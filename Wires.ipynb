{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f19fa14f-8d79-4759-8ec9-1f04bee37702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from enum import Enum\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "73d9698e-658f-4fcd-a55b-900fe909245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enums\n",
    "class Color(Enum):\n",
    "    BLACK = 0\n",
    "    RED = 1\n",
    "    BLUE = 2\n",
    "    YELLOW = 3\n",
    "    GREEN = 4\n",
    "\n",
    "class Label(Enum):\n",
    "    DANGEROUS = 0\n",
    "    SAFE = 1\n",
    "\n",
    "COLOR_TO_RGB = {\n",
    "    0 : (0, 0, 0),\n",
    "    1 : (255, 0, 0),\n",
    "    2 : (0, 0, 255),\n",
    "    3 : (255, 255, 0),\n",
    "    4 : (0, 255, 0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d52f086b-13da-454e-a629-8b2b154f8310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Generation\n",
    "class Image: # USE THIS CLASS TO DISPLAY / CREATE IMAGE\n",
    "    def __init__(self, data, label, third_wire):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.third_wire = third_wire\n",
    "\n",
    "    def display(self):\n",
    "        n = self.data.shape[0]\n",
    "        image_data = [[0 for _ in range(n)] for _ in range(n)]\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                curr_val = self.data[i][j]\n",
    "                image_data[i][j] = COLOR_TO_RGB[curr_val]\n",
    "\n",
    "        plt.imshow(image_data)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self):\n",
    "        self.image_data = []\n",
    "        self.labels = []\n",
    "        self.third_wires = []\n",
    "\n",
    "    def add_image(self, image, label, third_wire):\n",
    "        \"\"\"Add an image to the dataset using raw data\"\"\"\n",
    "        self.image_data.append(image)\n",
    "        self.labels.append(label.value)\n",
    "        self.third_wires.append(third_wire) \n",
    "\n",
    "class ImageGenerator:\n",
    "    \"\"\"Generates N (M x M) images and writes to data\"\"\"\n",
    "    def __init__(self, num_images, dimensions = 20, write_path = None, dataset = None, seed = None):\n",
    "        self.dimensions = dimensions\n",
    "        self.seed = seed\n",
    "        random.seed(seed)\n",
    "        for _ in range(num_images):\n",
    "            data, label, third_wire = self.generate(write_path is not None)\n",
    "            if dataset is not None:\n",
    "                dataset.add_image(data, label, third_wire)\n",
    "\n",
    "            # TODO: Write to data (may not be necessary, gen is really fast)\n",
    "            if write_path is not None:\n",
    "                pass\n",
    "\n",
    "    def generate(self, write_ = False):\n",
    "        \"\"\"Generate a single image and label it appropriately\"\"\"\n",
    "        n = self.dimensions\n",
    "        image_data = np.full((n, n), Color.BLACK.value)\n",
    "        colors = [Color.RED, Color.BLUE, Color.YELLOW, Color.GREEN]\n",
    "\n",
    "        # Keep track of rows/cols that can be used\n",
    "        valid_rows = list(range(n))\n",
    "        valid_cols = list(range(n))\n",
    "\n",
    "        # 1  = Color Row\n",
    "        # -1 = Color Col\n",
    "        pointer = random.choice([1, -1])\n",
    "        label = Label.SAFE\n",
    "        yellow_placed = False\n",
    "        third_wire = None\n",
    "\n",
    "        for i in range(4):\n",
    "            # Select a unused color\n",
    "            curr_color = random.choice(colors)\n",
    "            colors.remove(curr_color)\n",
    "\n",
    "            # Determine which should be cut (if dangerous)\n",
    "            if i == 2:\n",
    "                third_wire = curr_color\n",
    "            \n",
    "            # Handle marking an image as DANGEROUS if red is placed before yellow\n",
    "            yellow_placed = yellow_placed or curr_color == Color.YELLOW\n",
    "            if curr_color == Color.RED and not yellow_placed:\n",
    "                label = Label.DANGEROUS\n",
    "\n",
    "            if pointer == 1:\n",
    "                rand_idx = random.choice(valid_rows)\n",
    "                valid_rows.remove(rand_idx)\n",
    "                self.color_row(rand_idx, curr_color, image_data)\n",
    "            else:\n",
    "                rand_idx = random.choice(valid_cols)\n",
    "                valid_cols.remove(rand_idx)\n",
    "                self.color_column(rand_idx, curr_color, image_data)\n",
    "                \n",
    "            pointer *= -1\n",
    "        \n",
    "        return image_data, label, third_wire\n",
    "            \n",
    "\n",
    "    def color_column(self, col, color, arr):\n",
    "        \"\"\"Color the given column of the image with the provided color\"\"\"\n",
    "        n = self.dimensions\n",
    "        for i in range(n):\n",
    "            arr[i][col] = color.value\n",
    "\n",
    "    def color_row(self, row, color, arr):\n",
    "        \"\"\"Color the given row of the image with the provided color\"\"\"\n",
    "        n = self.dimensions\n",
    "        for i in range(n):\n",
    "            arr[row][i] = color.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c8558733-7132-4c86-9b2a-27e1f506fe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataSet()\n",
    "img_gen = ImageGenerator(50, dataset = data, seed = 718067190)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "68fb007b-bd58-41ba-b5ac-098a9a464fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_image(dataset_obj, index):\n",
    "    \"\"\"Load a specific index of the dataset\"\"\"\n",
    "    image_data = dataset_obj.image_data[index]\n",
    "    label = dataset_obj.labels[index]\n",
    "    third_wire = dataset_obj.third_wires[index]\n",
    "    img = Image(image_data, label, third_wire)\n",
    "    return img\n",
    "\n",
    "load_image(data, 20).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d6b7a3ac-2fd5-4625-ae8e-4cad1caaa172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Math Stuff\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "d0955d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression - Task 1\n",
    "class LogisticRegression:\n",
    "    def __init__(self, examples, labels, alpha, epsilon):\n",
    "        self.n = len(examples)                                # of training examples\n",
    "        self.d = len(examples[0])                             # of features\n",
    "        self.examples = np.c_[np.ones(self.n), examples]      # Add column of 1s for W_0\n",
    "        self.labels = labels                                  # Classification Labels\n",
    "        self.weights = np.zeros(self.d + 1)                # Current parameters / weights\n",
    "        self.lr = alpha                                    # Learning rate   \n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def predict(self, inds=None):\n",
    "        \"\"\"Compute h_w(x_i) for the provided weight values\"\"\"\n",
    "        if inds is None:\n",
    "            inds = np.arange(len(self.examples))\n",
    "        \n",
    "        dot_product = np.dot(self.examples[inds], self.weights)\n",
    "        return sigmoid(dot_product)\n",
    "\n",
    "\n",
    "    def loss(self, y, p):\n",
    "        \"\"\"Compute the current value of average loss based on predictions\"\"\"\n",
    "        return np.mean(-y * np.log(p) - (1 - y) * np.log(1 - p))\n",
    "\n",
    "    def gd(self):\n",
    "        \"\"\"Run Gradient Descent to find `parameters` to minimize loss\"\"\"\n",
    "        # Shuffle data before each epoch\n",
    "        # random.shuffle(self.examples)\n",
    "        # for i in range(len(self.examples)):\n",
    "        #errors = self.loss(self.labels, self.predict())\n",
    "        residuals = self.predict() - self.labels\n",
    "        gradient = np.dot(self.examples.T, residuals) / self.n\n",
    "        self.weights -= self.lr * gradient\n",
    "        \n",
    "    def sgd(self):\n",
    "        \"\"\"Run Stochastic Gradient Descent to find `parameters` to minimize loss\"\"\"\n",
    "        # Shuffle data before each epoch\n",
    "        indices_array = np.arange(len(self.examples))\n",
    "        random.shuffle(indices_array)\n",
    "        \n",
    "        for ind in indices_array:\n",
    "            residual = self.predict(ind) - self.labels[ind]\n",
    "            gradient = residual * self.examples[ind]\n",
    "            self.weights -= self.lr * gradient\n",
    "\n",
    "    def train_sgd(self, epochs):\n",
    "        prev = deque([float('inf')])\n",
    "        for epoch in range(epochs):\n",
    "            self.sgd()\n",
    "            current_loss = self.loss(self.labels, self.predict())\n",
    "            print(f\"{epoch} - Loss: {current_loss}\")\n",
    "            \n",
    "            rolling_mean = sum(prev)/len(prev)\n",
    "            if rolling_mean - current_loss < self.epsilon:\n",
    "                print(f\"Stopping early at epoch {epoch} - Loss: {current_loss}\")\n",
    "                break\n",
    "            prev.append(float(current_loss))\n",
    "            if len(prev) > 5:\n",
    "                prev.popleft()\n",
    "            \n",
    "    def train_gd(self, epochs):\n",
    "        prev = float('inf')\n",
    "        for epoch in range(epochs):\n",
    "            self.gd()\n",
    "            current_loss = self.loss(self.labels, self.predict())\n",
    "            print(f\"{epoch} - Loss: {current_loss}\")\n",
    "            if prev - current_loss < self.epsilon:\n",
    "                print(f\"Stopping early at epoch {epoch} - Loss: {current_loss}\")\n",
    "                break\n",
    "            prev = current_loss\n",
    "\n",
    "    # Regular GD with the same early stopping condition as SGD\n",
    "    \n",
    "    # def train_gd(self, epochs):\n",
    "    #     prev = deque([float('inf')])\n",
    "    #     for epoch in range(epochs):\n",
    "    #         self.gd()\n",
    "    #         current_loss = self.loss(self.labels, self.predict())\n",
    "    #         print(f\"{epoch} - Loss: {current_loss}\")\n",
    "    #         \n",
    "    #         rolling_mean = sum(prev)/len(prev)\n",
    "    #         if rolling_mean - current_loss < self.epsilon:\n",
    "    #             print(f\"Stopping early at epoch {epoch} - Loss: {current_loss}\")\n",
    "    #             break\n",
    "    #         prev.append(float(current_loss))\n",
    "    #         if len(prev) > 5:\n",
    "    #             prev.popleft()\n",
    "\n",
    "    def indicator(self, pred):\n",
    "        \"\"\"Returns label 1 if p(y == 1) > .5, 0 if p(y == 1) < .5, and breaks ties randomly\"\"\"\n",
    "        if pred > .5:\n",
    "            return 1\n",
    "        elif pred < 5:\n",
    "            return 0\n",
    "        return np.random.choice([0, 1])\n",
    "    \n",
    "    def get_pred_labels(self, preds):\n",
    "        \"\"\"Converts prediction probabilities into labels\"\"\"\n",
    "        for i in range(len(preds)):\n",
    "            preds[i] = self.indicator(preds[i])\n",
    "            \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "41752184-26ab-4070-b264-067b4ec876e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 3, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess data (this should be done somewhere else, but here for now)\n",
    "image_data = np.array(data.image_data)\n",
    "label_data = np.array(data.labels)\n",
    "flattened_data = image_data.reshape(image_data.shape[0], -1)\n",
    "flattened_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "501114df-ab19-47f5-a4ec-8fa1d087e78b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "       0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "       0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "       0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = .01\n",
    "epsilon = .001#.001\n",
    "\n",
    "logistic = LogisticRegression(flattened_data, label_data, lr, epsilon)\n",
    "predictions = logistic.predict()\n",
    "loss = logistic.loss(logistic.labels, predictions)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "fa529d9d-8e5b-444e-8bf9-6c55c636ce20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Loss: 0.6678100999116309\n",
      "1 - Loss: 0.6463966646615411\n",
      "2 - Loss: 0.6279321157436055\n",
      "3 - Loss: 0.6117524044218718\n",
      "4 - Loss: 0.5973923160863936\n",
      "5 - Loss: 0.5845147398250198\n",
      "6 - Loss: 0.572867126134453\n",
      "7 - Loss: 0.5622546770879252\n",
      "8 - Loss: 0.552523524073432\n",
      "9 - Loss: 0.5435498366926135\n",
      "10 - Loss: 0.5352325002008094\n",
      "11 - Loss: 0.52748799762632\n",
      "12 - Loss: 0.5202467031765879\n",
      "13 - Loss: 0.51345011568962\n",
      "14 - Loss: 0.5070487434103192\n",
      "15 - Loss: 0.5010004563767247\n",
      "16 - Loss: 0.49526918467480274\n",
      "17 - Loss: 0.489823878653017\n",
      "18 - Loss: 0.4846376712029767\n",
      "19 - Loss: 0.47968719808965204\n",
      "20 - Loss: 0.47495204322635204\n",
      "21 - Loss: 0.47041428354930853\n",
      "22 - Loss: 0.4660581138231741\n",
      "23 - Loss: 0.46186953595520736\n",
      "24 - Loss: 0.45783610062782637\n",
      "25 - Loss: 0.45394669155132583\n",
      "26 - Loss: 0.4501913445795446\n",
      "27 - Loss: 0.4465610954548016\n",
      "28 - Loss: 0.44304785115174705\n",
      "29 - Loss: 0.4396442807451107\n",
      "30 - Loss: 0.4363437224881782\n",
      "31 - Loss: 0.43314010439878087\n",
      "32 - Loss: 0.43002787613972565\n",
      "33 - Loss: 0.4270019503758195\n",
      "34 - Loss: 0.4240576521093847\n",
      "35 - Loss: 0.4211906747556664\n",
      "36 - Loss: 0.4183970419308208\n",
      "37 - Loss: 0.415673074097721\n",
      "38 - Loss: 0.41301535935616285\n",
      "39 - Loss: 0.4104207277801956\n",
      "40 - Loss: 0.4078862288009973\n",
      "41 - Loss: 0.4054091112128191\n",
      "42 - Loss: 0.40298680544508536\n",
      "43 - Loss: 0.4006169077982333\n",
      "44 - Loss: 0.39829716638631163\n",
      "45 - Loss: 0.3960254685673354\n",
      "46 - Loss: 0.3937998296742308\n",
      "47 - Loss: 0.3916183828859631\n",
      "48 - Loss: 0.3894793701009936\n",
      "49 - Loss: 0.387381133694271\n",
      "50 - Loss: 0.385322109055111\n",
      "51 - Loss: 0.3833008178170315\n",
      "52 - Loss: 0.38131586170229725\n",
      "53 - Loss: 0.3793659169138981\n",
      "54 - Loss: 0.3774497290162241\n",
      "55 - Loss: 0.37556610825302444\n",
      "56 - Loss: 0.37371392525753905\n",
      "57 - Loss: 0.37189210711512416\n",
      "58 - Loss: 0.3700996337433849\n",
      "59 - Loss: 0.36833553455890383\n",
      "60 - Loss: 0.3665988854031745\n",
      "61 - Loss: 0.3648888057034321\n",
      "62 - Loss: 0.3632044558467536\n",
      "63 - Loss: 0.3615450347481364\n",
      "64 - Loss: 0.3599097775953283\n",
      "65 - Loss: 0.3582979537549795\n",
      "66 - Loss: 0.3567088648262737\n",
      "67 - Loss: 0.35514184282959943\n",
      "68 - Loss: 0.35359624851905475\n",
      "69 - Loss: 0.3520714698086765\n",
      "70 - Loss: 0.3505669203032564\n",
      "71 - Loss: 0.34908203792546944\n",
      "72 - Loss: 0.3476162836318114\n",
      "73 - Loss: 0.346169140210526\n",
      "74 - Loss: 0.3447401111553212\n",
      "75 - Loss: 0.3433287196092184\n",
      "76 - Loss: 0.34193450737337805\n",
      "77 - Loss: 0.3405570339761833\n",
      "78 - Loss: 0.3391958757982688\n",
      "79 - Loss: 0.33785062524953574\n",
      "80 - Loss: 0.33652088999452706\n",
      "81 - Loss: 0.33520629222282494\n",
      "82 - Loss: 0.333906467961404\n",
      "83 - Loss: 0.33262106642611555\n",
      "84 - Loss: 0.33134974940969586\n",
      "85 - Loss: 0.33009219070389556\n",
      "86 - Loss: 0.3288480755535082\n",
      "87 - Loss: 0.3276171001402426\n",
      "88 - Loss: 0.3263989710945377\n",
      "89 - Loss: 0.3251934050335554\n",
      "90 - Loss: 0.3240001281237188\n",
      "91 - Loss: 0.32281887566627576\n",
      "92 - Loss: 0.32164939170447904\n",
      "93 - Loss: 0.32049142865107144\n",
      "94 - Loss: 0.31934474693485554\n",
      "95 - Loss: 0.31820911466521146\n",
      "96 - Loss: 0.3170843073135048\n",
      "97 - Loss: 0.31597010741039516\n",
      "98 - Loss: 0.3148663042581253\n",
      "99 - Loss: 0.31377269365692784\n",
      "100 - Loss: 0.31268907764474746\n",
      "101 - Loss: 0.3116152642495242\n",
      "102 - Loss: 0.3105510672533347\n",
      "103 - Loss: 0.30949630596773214\n",
      "104 - Loss: 0.30845080501966743\n",
      "105 - Loss: 0.307414394147413\n",
      "106 - Loss: 0.3063869080059458\n",
      "107 - Loss: 0.3053681859812811\n",
      "108 - Loss: 0.304358072013278\n",
      "109 - Loss: 0.30335641442646805\n",
      "110 - Loss: 0.30236306576848376\n",
      "111 - Loss: 0.30137788265569215\n",
      "112 - Loss: 0.30040072562565767\n",
      "113 - Loss: 0.29943145899608387\n",
      "114 - Loss: 0.2984699507299034\n",
      "115 - Loss: 0.29751607230620375\n",
      "116 - Loss: 0.29656969859669574\n",
      "117 - Loss: 0.2956307077474464\n",
      "118 - Loss: 0.29469898106561687\n",
      "119 - Loss: 0.29377440291095674\n",
      "120 - Loss: 0.2928568605918232\n",
      "121 - Loss: 0.2919462442655052\n",
      "122 - Loss: 0.2910424468426436\n",
      "123 - Loss: 0.290145363895553\n",
      "124 - Loss: 0.28925489357025796\n",
      "125 - Loss: 0.2883709365020677\n",
      "126 - Loss: 0.2874933957345254\n",
      "127 - Loss: 0.2866221766415723\n",
      "128 - Loss: 0.28575718685277807\n",
      "129 - Loss: 0.2848983361814982\n",
      "130 - Loss: 0.28404553655582193\n",
      "131 - Loss: 0.2831987019521863\n",
      "132 - Loss: 0.2823577483315342\n",
      "133 - Loss: 0.2815225935779035\n",
      "134 - Loss: 0.280693157439339\n",
      "135 - Loss: 0.2798693614710236\n",
      "136 - Loss: 0.2790511289805322\n",
      "137 - Loss: 0.2782383849751144\n",
      "138 - Loss: 0.27743105611091906\n",
      "139 - Loss: 0.2766290706440768\n",
      "140 - Loss: 0.2758323583835598\n",
      "141 - Loss: 0.27504085064574424\n",
      "142 - Loss: 0.2742544802106033\n",
      "143 - Loss: 0.2734731812794617\n",
      "144 - Loss: 0.2726968894342465\n",
      "145 - Loss: 0.2719255415981726\n",
      "146 - Loss: 0.27115907599780287\n",
      "147 - Loss: 0.2703974321264269\n",
      "148 - Loss: 0.26964055070870563\n",
      "149 - Loss: 0.2688883736665275\n",
      "150 - Loss: 0.2681408440860322\n",
      "151 - Loss: 0.2673979061857495\n",
      "152 - Loss: 0.26665950528581356\n",
      "153 - Loss: 0.2659255877782077\n",
      "154 - Loss: 0.26519610109799957\n",
      "155 - Loss: 0.2644709936955283\n",
      "156 - Loss: 0.2637502150095061\n",
      "157 - Loss: 0.26303371544099924\n",
      "158 - Loss: 0.2623214463282554\n",
      "159 - Loss: 0.26161335992234297\n",
      "160 - Loss: 0.26090940936357376\n",
      "161 - Loss: 0.2602095486586784\n",
      "162 - Loss: 0.2595137326587056\n",
      "163 - Loss: 0.2588219170376199\n",
      "164 - Loss: 0.2581340582715704\n",
      "165 - Loss: 0.2574501136188061\n",
      "166 - Loss: 0.25677004110021495\n",
      "167 - Loss: 0.25609379948046285\n",
      "168 - Loss: 0.25542134824971086\n",
      "169 - Loss: 0.25475264760589034\n",
      "170 - Loss: 0.2540876584375158\n",
      "171 - Loss: 0.25342634230701583\n",
      "172 - Loss: 0.25276866143456367\n",
      "173 - Loss: 0.25211457868239123\n",
      "174 - Loss: 0.25146405753956697\n",
      "175 - Loss: 0.2508170621072238\n",
      "176 - Loss: 0.2501735570842204\n",
      "177 - Loss: 0.24953350775322036\n",
      "178 - Loss: 0.24889687996717616\n",
      "179 - Loss: 0.24826364013620278\n",
      "180 - Loss: 0.24763375521482855\n",
      "181 - Loss: 0.2470071926896102\n",
      "182 - Loss: 0.24638392056709968\n",
      "183 - Loss: 0.245763907362151\n",
      "184 - Loss: 0.24514712208655612\n",
      "185 - Loss: 0.24453353423799856\n",
      "186 - Loss: 0.2439231137893142\n",
      "187 - Loss: 0.2433158311780494\n",
      "188 - Loss: 0.2427116572963072\n",
      "189 - Loss: 0.24211056348087062\n",
      "190 - Loss: 0.2415125215035964\n",
      "191 - Loss: 0.2409175035620682\n",
      "192 - Loss: 0.24032548227050274\n",
      "193 - Loss: 0.23973643065089967\n",
      "194 - Loss: 0.23915032212442758\n",
      "195 - Loss: 0.23856713050304\n",
      "196 - Loss: 0.2379868299813117\n",
      "197 - Loss: 0.23740939512849096\n",
      "198 - Loss: 0.2368348008807593\n",
      "199 - Loss: 0.23626302253369297\n",
      "200 - Loss: 0.2356940357349201\n",
      "201 - Loss: 0.23512781647696723\n",
      "202 - Loss: 0.23456434109028923\n",
      "203 - Loss: 0.23400358623647777\n",
      "204 - Loss: 0.233445528901642\n",
      "205 - Loss: 0.23289014638995706\n",
      "206 - Loss: 0.23233741631737487\n",
      "207 - Loss: 0.23178731660549282\n",
      "208 - Loss: 0.23123982547557503\n",
      "209 - Loss: 0.23069492144272202\n",
      "210 - Loss: 0.23015258331018487\n",
      "211 - Loss: 0.22961279016381902\n",
      "212 - Loss: 0.22907552136667322\n",
      "213 - Loss: 0.22854075655371145\n",
      "214 - Loss: 0.2280084756266616\n",
      "215 - Loss: 0.22747865874898954\n",
      "216 - Loss: 0.2269512863409928\n",
      "217 - Loss: 0.226426339075013\n",
      "218 - Loss: 0.22590379787076045\n",
      "219 - Loss: 0.2253836438907515\n",
      "220 - Loss: 0.2248658585358515\n",
      "221 - Loss: 0.2243504234409242\n",
      "222 - Loss: 0.22383732047058116\n",
      "223 - Loss: 0.22332653171503164\n",
      "224 - Loss: 0.22281803948602724\n",
      "225 - Loss: 0.2223118263129008\n",
      "226 - Loss: 0.22180787493869583\n",
      "227 - Loss: 0.22130616831638478\n",
      "228 - Loss: 0.22080668960517266\n",
      "229 - Loss: 0.2203094221668853\n",
      "230 - Loss: 0.21981434956243773\n",
      "231 - Loss: 0.21932145554838314\n",
      "232 - Loss: 0.218830724073538\n",
      "233 - Loss: 0.21834213927568263\n",
      "234 - Loss: 0.21785568547833495\n",
      "235 - Loss: 0.21737134718759474\n",
      "236 - Loss: 0.21688910908905804\n",
      "237 - Loss: 0.21640895604479837\n",
      "238 - Loss: 0.21593087309041373\n",
      "239 - Loss: 0.2154548454321376\n",
      "240 - Loss: 0.21498085844401246\n",
      "241 - Loss: 0.21450889766512368\n",
      "242 - Loss: 0.21403894879689261\n",
      "243 - Loss: 0.21357099770042764\n",
      "244 - Loss: 0.21310503039393092\n",
      "245 - Loss: 0.21264103305016005\n",
      "246 - Loss: 0.21217899199394336\n",
      "247 - Loss: 0.21171889369974642\n",
      "248 - Loss: 0.2112607247892903\n",
      "249 - Loss: 0.21080447202921854\n",
      "250 - Loss: 0.21035012232881203\n",
      "251 - Loss: 0.20989766273775196\n",
      "252 - Loss: 0.2094470804439275\n",
      "253 - Loss: 0.20899836277128894\n",
      "254 - Loss: 0.20855149717774396\n",
      "255 - Loss: 0.20810647125309673\n",
      "256 - Loss: 0.20766327271702834\n",
      "257 - Loss: 0.20722188941711792\n",
      "258 - Loss: 0.2067823093269028\n",
      "259 - Loss: 0.20634452054397823\n",
      "260 - Loss: 0.20590851128813334\n",
      "261 - Loss: 0.20547426989952514\n",
      "262 - Loss: 0.20504178483688767\n",
      "263 - Loss: 0.20461104467577645\n",
      "264 - Loss: 0.20418203810684665\n",
      "265 - Loss: 0.2037547539341655\n",
      "266 - Loss: 0.20332918107355652\n",
      "267 - Loss: 0.2029053085509758\n",
      "268 - Loss: 0.2024831255009197\n",
      "269 - Loss: 0.20206262116486248\n",
      "270 - Loss: 0.20164378488972381\n",
      "271 - Loss: 0.2012266061263654\n",
      "272 - Loss: 0.20081107442811558\n",
      "273 - Loss: 0.200397179449322\n",
      "274 - Loss: 0.19998491094393106\n",
      "275 - Loss: 0.19957425876409382\n",
      "276 - Loss: 0.19916521285879782\n",
      "277 - Loss: 0.1987577632725243\n",
      "278 - Loss: 0.19835190014392962\n",
      "279 - Loss: 0.1979476137045513\n",
      "280 - Loss: 0.1975448942775376\n",
      "281 - Loss: 0.19714373227639978\n",
      "282 - Loss: 0.1967441182037873\n",
      "283 - Loss: 0.19634604265028519\n",
      "284 - Loss: 0.19594949629323216\n",
      "285 - Loss: 0.19555446989556122\n",
      "286 - Loss: 0.19516095430465938\n",
      "287 - Loss: 0.194768940451249\n",
      "288 - Loss: 0.19437841934828787\n",
      "289 - Loss: 0.19398938208988867\n",
      "290 - Loss: 0.19360181985025815\n",
      "291 - Loss: 0.1932157238826542\n",
      "292 - Loss: 0.19283108551836092\n",
      "293 - Loss: 0.19244789616568167\n",
      "294 - Loss: 0.19206614730894944\n",
      "295 - Loss: 0.1916858305075545\n",
      "296 - Loss: 0.19130693739498794\n",
      "297 - Loss: 0.19092945967790215\n",
      "298 - Loss: 0.19055338913518682\n",
      "299 - Loss: 0.19017871761706112\n",
      "300 - Loss: 0.18980543704418015\n",
      "301 - Loss: 0.1894335394067577\n",
      "302 - Loss: 0.18906301676370227\n",
      "303 - Loss: 0.18869386124176846\n",
      "304 - Loss: 0.188326065034722\n",
      "305 - Loss: 0.18795962040251857\n",
      "306 - Loss: 0.18759451967049634\n",
      "307 - Loss: 0.1872307552285811\n",
      "308 - Loss: 0.18686831953050526\n",
      "309 - Loss: 0.18650720509303884\n",
      "310 - Loss: 0.18614740449523315\n",
      "311 - Loss: 0.18578891037767659\n",
      "312 - Loss: 0.18543171544176232\n",
      "313 - Loss: 0.18507581244896806\n",
      "314 - Loss: 0.18472119422014668\n",
      "315 - Loss: 0.18436785363482855\n",
      "316 - Loss: 0.1840157836305349\n",
      "317 - Loss: 0.18366497720210145\n",
      "318 - Loss: 0.18331542740101348\n",
      "319 - Loss: 0.1829671273347505\n",
      "320 - Loss: 0.18262007016614168\n",
      "321 - Loss: 0.18227424911273102\n",
      "322 - Loss: 0.1819296574461522\n",
      "323 - Loss: 0.18158628849151318\n",
      "324 - Loss: 0.1812441356267906\n",
      "325 - Loss: 0.1809031922822324\n",
      "326 - Loss: 0.18056345193977097\n",
      "327 - Loss: 0.18022490813244368\n",
      "328 - Loss: 0.17988755444382332\n",
      "329 - Loss: 0.17955138450745636\n",
      "330 - Loss: 0.17921639200631018\n",
      "331 - Loss: 0.17888257067222801\n",
      "332 - Loss: 0.1785499142853924\n",
      "333 - Loss: 0.17821841667379645\n",
      "Stopping early at epoch 333 - Loss: 0.17821841667379645\n"
     ]
    }
   ],
   "source": [
    "gd = logistic.train_gd(20000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "7c97b88b-73bb-47f0-80f0-623d54b66b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
       "       1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = logistic.predict()\n",
    "\n",
    "# for i in range(len(predictions)):\n",
    "#     if predictions[i] < 0.5:\n",
    "#         predictions[i] = int(0)\n",
    "#     else:\n",
    "#         predictions[i] = int(1)\n",
    "predictions = logistic.get_pred_labels(predictions)\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "53310376-4680-4735-af25-ce9e8c6df641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff30abf-361c-4277-a521-7f94cad36bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
