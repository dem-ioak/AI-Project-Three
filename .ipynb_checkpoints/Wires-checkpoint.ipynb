{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff11b85a-e34d-47e2-a032-04b20ff0b11e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Dependencies\n",
    "All modules and packages required for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19fa14f-8d79-4759-8ec9-1f04bee37702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from enum import Enum\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c44219-0f72-4f0f-aca7-7b3537e4b42b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Enums\n",
    "Enums used for image generation and image display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d9698e-658f-4fcd-a55b-900fe909245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enums\n",
    "class Color(Enum):\n",
    "    RED = 0\n",
    "    BLUE = 1\n",
    "    YELLOW = 2\n",
    "    GREEN = 3\n",
    "    BLACK = 4\n",
    "\n",
    "class Label(Enum):\n",
    "    DANGEROUS = 0\n",
    "    SAFE = 1\n",
    "\n",
    "COLOR_TO_RGB = {\n",
    "    0 : (255, 0, 0),\n",
    "    1 : (0, 0, 255),\n",
    "    2 : (255, 255, 0),\n",
    "    3 : (0, 255, 0),\n",
    "    4 : (0, 0, 0)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aa201e-7b0e-44af-939c-a6f3e2338708",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Image generation\n",
    "This handles all image generation tasks, as well as provides a framework to allow display of generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52f086b-13da-454e-a629-8b2b154f8310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Generation\n",
    "class Image: # USE THIS CLASS TO DISPLAY / CREATE IMAGE\n",
    "    def __init__(self, data, label, third_wire):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.third_wire = third_wire\n",
    "\n",
    "    def display(self):\n",
    "        n = self.data.shape[0]\n",
    "        image_data = [[0 for _ in range(n)] for _ in range(n)]\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                curr_val = self.data[i][j]\n",
    "                image_data[i][j] = COLOR_TO_RGB[curr_val]\n",
    "\n",
    "        plt.imshow(image_data)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self):\n",
    "        self.image_data = []\n",
    "        self.labels = []\n",
    "        self.third_wires = []\n",
    "\n",
    "    def add_image(self, image, label, third_wire):\n",
    "        \"\"\"Add an image to the dataset using raw data\"\"\"\n",
    "        self.image_data.append(image)\n",
    "        self.labels.append(label.value)\n",
    "        self.third_wires.append(third_wire) \n",
    "\n",
    "class ImageGenerator:\n",
    "    \"\"\"Generates N (M x M) images and writes to data\"\"\"\n",
    "    def __init__(self, num_images, dimensions = 20, write_path = None, dataset = None, seed = None):\n",
    "        self.dimensions = dimensions\n",
    "        self.seed = seed\n",
    "        random.seed(seed)\n",
    "        for _ in range(num_images):\n",
    "            data, label, third_wire = self.generate(write_path is not None)\n",
    "            if dataset is not None:\n",
    "                dataset.add_image(data, label, third_wire)\n",
    "\n",
    "            # TODO: Write to data (may not be necessary, gen is really fast)\n",
    "            if write_path is not None:\n",
    "                pass\n",
    "\n",
    "    def generate(self, write_ = False):\n",
    "        \"\"\"Generate a single image and label it appropriately\"\"\"\n",
    "        n = self.dimensions\n",
    "        image_data = np.full((n, n), Color.BLACK.value)\n",
    "        colors = [Color.RED, Color.BLUE, Color.YELLOW, Color.GREEN]\n",
    "\n",
    "        # Keep track of rows/cols that can be used\n",
    "        valid_rows = list(range(n))\n",
    "        valid_cols = list(range(n))\n",
    "\n",
    "        # 1  = Color Row\n",
    "        # -1 = Color Col\n",
    "        pointer = random.choice([1, -1])\n",
    "        label = Label.SAFE\n",
    "        yellow_placed = False\n",
    "        third_wire = None\n",
    "\n",
    "        for i in range(4):\n",
    "            # Select a unused color\n",
    "            curr_color = random.choice(colors)\n",
    "            colors.remove(curr_color)\n",
    "\n",
    "            # Determine which should be cut (if dangerous)\n",
    "            if i == 2:\n",
    "                third_wire = curr_color\n",
    "            \n",
    "            # Handle marking an image as DANGEROUS if red is placed before yellow\n",
    "            yellow_placed = yellow_placed or curr_color == Color.YELLOW\n",
    "            if curr_color == Color.RED and not yellow_placed:\n",
    "                label = Label.DANGEROUS\n",
    "\n",
    "            if pointer == 1:\n",
    "                rand_idx = random.choice(valid_rows)\n",
    "                valid_rows.remove(rand_idx)\n",
    "                self.color_row(rand_idx, curr_color, image_data)\n",
    "            else:\n",
    "                rand_idx = random.choice(valid_cols)\n",
    "                valid_cols.remove(rand_idx)\n",
    "                self.color_column(rand_idx, curr_color, image_data)\n",
    "                \n",
    "            pointer *= -1\n",
    "        \n",
    "        return image_data, label, third_wire\n",
    "            \n",
    "\n",
    "    def color_column(self, col, color, arr):\n",
    "        \"\"\"Color the given column of the image with the provided color\"\"\"\n",
    "        n = self.dimensions\n",
    "        for i in range(n):\n",
    "            arr[i][col] = color.value\n",
    "\n",
    "    def color_row(self, row, color, arr):\n",
    "        \"\"\"Color the given row of the image with the provided color\"\"\"\n",
    "        n = self.dimensions\n",
    "        for i in range(n):\n",
    "            arr[row][i] = color.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aadfef-ed1b-4bf9-92de-9ef92981d3a6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Helper Methods\n",
    "Useful methods that compartmentalize logic for preprocessing and model behavior, as well as plotting results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c49a5c-64ff-4c9d-840e-3b2e173d805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def load_image(dataset_obj, index):\n",
    "    \"\"\"Load a specific index of the dataset\"\"\"\n",
    "    image_data = dataset_obj.image_data[index]\n",
    "    label = dataset_obj.labels[index]\n",
    "    third_wire = dataset_obj.third_wires[index]\n",
    "    img = Image(image_data, label, third_wire)\n",
    "    return img\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def train_test_validation_split(X, y, train_size = .8, test_size = .1, validation_size = .1):\n",
    "    if train_size + test_size + validation_size != 1:\n",
    "        print(\"Error: train + test + validation don't add up to 1.\")\n",
    "        return\n",
    "    \n",
    "    examples, features = X.shape\n",
    "    # Randomize Indexes\n",
    "    index_array = np.arange(examples)\n",
    "    random.shuffle(index_array)\n",
    "\n",
    "    # Split up data\n",
    "    num_training = int(examples * train_size)\n",
    "    num_testing = int(examples * test_size)\n",
    "    train_indices = index_array[:num_training]\n",
    "    test_indices = index_array[num_training:(num_training + num_testing)]\n",
    "    validation_indices = index_array[(num_training + num_testing):]\n",
    "    \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    X_validation = X[validation_indices]\n",
    "    y_validation = y[validation_indices]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, X_validation, y_validation\n",
    "    \n",
    "\n",
    "def one_hot_encode(image_data):\n",
    "    \"\"\"One-Hot encode matrix of examples\"\"\"\n",
    "    if not isinstance(image_data, np.ndarray):\n",
    "        image_data = np.array(image_data)\n",
    "\n",
    "    num_colors = 4\n",
    "    examples, rows, cols = image_data.shape\n",
    "    encoded_shape = (examples, rows, cols, num_colors)\n",
    "    encoded_image = np.zeros(encoded_shape)\n",
    "    for i in range(examples):\n",
    "        for j in range(rows):\n",
    "            for k in range(cols):\n",
    "                for l in range(num_colors):\n",
    "                    if l == image_data[i, j, k]:\n",
    "                        encoded_image[i, j, k, l] = 1\n",
    "                    \n",
    "    return encoded_image\n",
    "\n",
    "def apply_convolution(image_data, step_size = 1):\n",
    "    \"\"\"Apply convolution\"\"\"\n",
    "    num_images, W, _, C = image_data.shape\n",
    "    \n",
    "    # Kernel Initialization\n",
    "    kernel = np.array([[0, 1, 0], [1, 1, 1], [0, 1, 0]])\n",
    "    K = 3\n",
    "\n",
    "    output_size = ((W - K) // step_size) + 1\n",
    "    output = np.zeros((num_images, output_size, output_size, C))\n",
    "\n",
    "    # For every image\n",
    "    for n in range(num_images):\n",
    "\n",
    "        # For every element of the feature map\n",
    "        for i in range(output_size):\n",
    "            for j in range(output_size):\n",
    "                region = image_data[n, i : i + K, j : j + K] # (3, 3, 4) - pixel subregion\n",
    "                for c in range(C):\n",
    "                    output[n, i, j, c] += np.sum(kernel * region[:, :, c])\n",
    "    return output\n",
    "\n",
    "def apply_pooling(image_data, P, mode_ = \"max\"):\n",
    "    if mode_ not in [\"max\", \"mean\"]:\n",
    "        return -1\n",
    "\n",
    "    num_images, W, _, C = image_data.shape\n",
    "    output_size = W // P\n",
    "    output = np.zeros((num_images, output_size, output_size, C))\n",
    "\n",
    "    for n in range(num_images):\n",
    "        for i in range(0, W - P + 1, P):\n",
    "            for j in range(0, W - P + 1, P):\n",
    "                region = image_data[n, i : i + P, j : j + P, :] # (3, 3, 4) - pixel subregion\n",
    "                for c in range(C):\n",
    "                    func_ = np.max if mode_ == \"max\" else np.mean\n",
    "                    output[n, i // P, j // P, c] = func_(region[:, :, c])\n",
    "    return output\n",
    "\n",
    "def row_col_features(image_data):\n",
    "    col_features = np.sum(image_data, axis=1)\n",
    "    row_features = np.sum(image_data, axis=2)\n",
    "    return np.concatenate([col_features, row_features], axis=1)\n",
    "\n",
    "def preprocess_data_plus(image_data, label_data, processing=None):\n",
    "    \"\"\"Uses neither convolution or row_col_features by default (only one_hot encoding)\n",
    "    \n",
    "    - Use \"convolution\" for convolution\n",
    "    - Use \"rc for row_col_features\n",
    "    - Use \"both\" for both of those features\"\"\"\n",
    "    image_data = one_hot_encode(image_data)\n",
    "    label_data = np.array(label_data)\n",
    "    flattened_raw = image_data.reshape(image_data.shape[0], -1)\n",
    "    output = np.concatenate([flattened_raw], axis = 1)\n",
    "    \n",
    "    if processing in (\"convolution\", \"both\"):\n",
    "        convolved = apply_convolution(image_data, 1)\n",
    "        pooled = apply_pooling(convolved, 2, \"mean\")\n",
    "        flattened_pooled = pooled.reshape(pooled.shape[0], -1)\n",
    "        flattened_pooled_squared = np.square(flattened_pooled)  \n",
    "        output = np.concatenate([flattened_raw, flattened_pooled, flattened_pooled_squared], axis = 1)\n",
    "    if processing in (\"rc\", \"both\"):\n",
    "        rc_features = row_col_features(image_data)\n",
    "        flattened_rc = rc_features.reshape(rc_features.shape[0], -1)\n",
    "        flattened_rc_squared = np.square(flattened_rc)\n",
    "        output = np.concatenate([flattened_raw, flattened_rc, flattened_rc_squared], axis = 1)\n",
    "    if processing == \"both\":\n",
    "        output = np.concatenate([flattened_raw, flattened_pooled, flattened_pooled_squared, flattened_rc, flattened_rc_squared], axis = 1)\n",
    "        \n",
    "    flattened_data = np.c_[np.ones(len(output)), output] \n",
    "    return flattened_data, label_data\n",
    "    \n",
    "def plot_data(title, x_label, y_label, data):\n",
    "    \"\"\"Data should be either a list or tuple, where the list is a list of tuples.\n",
    "    \n",
    "    Tuple format: Data to be plotted, name of data\"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    if isinstance(data, list):\n",
    "        for d in data:\n",
    "            ax.plot(d[0], label = d[1])\n",
    "    else:\n",
    "        ax.plot(data[0], label = data[1])\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(x_label) \n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aee4db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Task 1 \n",
    "\n",
    "Build and train a model on the data set to take as input a wiring diagram and give as output whether or not it is dangerous.\n",
    "\n",
    "- How are you defining your input space?\n",
    "  - 20x20 image -> 400-dimension vector (one for each pixel)\n",
    "    - For each pixel, use one-hot encoding for each possible color (= 1600 features)\n",
    "  - Convolution layer converting image to 18x18 grid\n",
    "    - Use mean pooling on convolutional grid to get 9x9 grid \n",
    "      - For each cell, four possible colors (9x9x4 = 324 features)\n",
    "  - W_0 bias term (1 feature)\n",
    "  - 1600 + 324 + 1 = 1925 features\n",
    "- How are you defining your output space?\n",
    "  - Take sigmoid of the predictions (dot product of weights and data) to get a probability from 0 - 1.\n",
    "  - Use 0.5 as classification threshold (as half will be safe with the other half dangeous) to get a binary classification.\n",
    "  - 0 = Dangerous; 1 = Safe\n",
    "- What model space are you considering, and what parameters does it have? Be sure to specify any design choices you make here.\n",
    "  - Use Logistic Regression. \n",
    "  - Parameters: 1 Bias Term + 1600 raw features (cell and color) + 324 convolutional features\n",
    "    - We use convolutional features as it allows the model to better understand what is going on around a specific pixel. \n",
    "    - We used a cross shaped kernel to emphasize instances where a wire may intersect another wire, which would help in determining the situations where a diagram is dangerous\n",
    "      - The kernal was 3x3 as the model only needs to see its direct neighbors\n",
    "    - We then condense the data using mean pooling to understand how many colors may be in the surrounding area (better understand intersections)\n",
    "- How are you measuring the loss or error of a given model?\n",
    "  - Binary Cross Entropy/Log Loss\n",
    "- What training algorithm are you using to find the best model you can? Include any necessary math to specify your algorithm.\n",
    "  - Stochastic Gradient Descent. \n",
    "    - Take the derivative of the loss with respect to the weights for one specific datapoint.\n",
    "- How are you preventing overfitting?\n",
    "  - L2 Regularization to allow the gradient to explore different possible minima in the hopes to find the global minimum\n",
    "    - The penalty also has a decay so that as the model goes through more epochs, it stabilizes onto a specific minimum.\n",
    "  - Epsilon hyperparameter which measures how the losses are in respect to the number of epochs\n",
    "    - If the difference between the average of the last n losses (where n = 5) and the current loss was less than epsilon, we ended there.\n",
    "  - Train/Test/Validation split of 80/10/10 to prevent model overfitting to input data\n",
    "    - This ensures that the model is not only learning the data, but also generalizes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0955d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression - Task 1\n",
    "class LogisticRegression:\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, X_val, y_val, lr, epsilon, regularization):\n",
    "        self.n = len(X_train)                                           # of training examples\n",
    "        self.d = len(X_train[0])                                        # of features\n",
    "        self.X_train = X_train #np.c_[np.ones(self.n), X_train]         # Training data\n",
    "        self.X_test = X_test                                            # Testing data\n",
    "        self.X_val = X_val                                              # Validation data\n",
    "        self.y_train = y_train                                          # Training classification Labels\n",
    "        self.y_test = y_test                                            # Testing classification Labels\n",
    "        self.y_val = y_val                                              # Validation classification Labels\n",
    "        self.weights = np.zeros(self.d)                                 # Current parameters / weights with d rows\n",
    "        self.lr = lr                                                    # Learning rate   \n",
    "        self.epsilon = epsilon                                          # Early stopping difference\n",
    "        self.regularization, self.Lambda, self.decay = regularization   # Type of regularization, penalty, and decay of the penalty\n",
    "\n",
    "    # Helper methods \n",
    "    # dataset = 0 - train; 1 - val; 2 - test\n",
    "    def dataset_picker(self, dataset = 0):\n",
    "        if dataset == 0:\n",
    "            return self.X_train, self.y_train\n",
    "        elif dataset == 1:\n",
    "            return self.X_test, self.y_test\n",
    "        else:\n",
    "            return self.X_val, self.y_val\n",
    "\n",
    "    # Helper methods \n",
    "    def predict(self, inds=None, dataset = 0):\n",
    "        \"\"\"Compute h_w(x_i) for the provided weight values\"\"\"\n",
    "        X, y = self.dataset_picker(dataset)\n",
    "        if inds is None:\n",
    "            inds = np.arange(len(X))\n",
    "        \n",
    "        dot_product = np.dot(X[inds], self.weights)\n",
    "        return sigmoid(dot_product)\n",
    "\n",
    "    def loss(self, y, p):\n",
    "        \"\"\"Compute the current value of average loss based on predictions\"\"\"\n",
    "        buffer = 1e-15\n",
    "        loss = np.mean(-y * np.log(p + buffer) - (1 - y) * np.log(1 - p + buffer))\n",
    "        if self.regularization == 2:\n",
    "            loss += sum(self.Lambda * np.square(self.weights))\n",
    "        return loss\n",
    "    \n",
    "    def accuracy(self, gold_labels, preds):\n",
    "        pred_labels = self.get_pred_labels(preds)\n",
    "        correct = [1 if pred == gold else 0 for pred, gold in zip(pred_labels, gold_labels)]\n",
    "        count, total = sum(correct), len(correct)\n",
    "        acc = round(count/total*100, 2)\n",
    "        \n",
    "        return acc, count, total\n",
    "    \n",
    "    def predict_loss_acc(self, inds=None, dataset=0):\n",
    "        X, y = self.dataset_picker(dataset)\n",
    "        preds = self.predict(inds, dataset)\n",
    "\n",
    "        loss = self.loss(y, preds)\n",
    "        acc, correct, total = self.accuracy(y, preds)\n",
    "        \n",
    "        return loss, acc\n",
    "            \n",
    "    def sgd(self):\n",
    "        \"\"\"Run a single iteration of SGD\"\"\"\n",
    "        # Shuffle data before each epoch\n",
    "        indices_array = np.arange(len(self.X_train))\n",
    "        random.shuffle(indices_array)\n",
    "        \n",
    "        for ind in indices_array:\n",
    "            residual = self.predict(ind) - self.y_train[ind]\n",
    "            gradient = residual * self.X_train[ind]\n",
    "            if self.regularization == 2:\n",
    "                gradient += 2 * self.Lambda * self.weights\n",
    "            self.weights -= self.lr * gradient\n",
    "\n",
    "    # Stochastic Gradient Descent\n",
    "    def train_stochastic(self, epochs, display_steps = 1):\n",
    "        \"\"\"Run SGD until # of epochs is exceeded OR convergence\"\"\"\n",
    "        prev_loss = deque([float('inf')])\n",
    "        prev_acc = deque([float('inf')])\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "        print(\"Epoch\\t\\tTrainLoss\\tValLoss\\t\\tTrainAcc\\tValAcc\")  \n",
    "        for epoch in range(epochs):\n",
    "            self.sgd()\n",
    "\n",
    "            loss_train, acc_train = self.predict_loss_acc(dataset=0)\n",
    "            loss_val, acc_val = self.predict_loss_acc(dataset=2)\n",
    "            \n",
    "            self.train_losses.append(loss_train)\n",
    "            self.val_losses.append(loss_val)\n",
    "            self.train_accuracies.append(acc_train)\n",
    "            self.val_accuracies.append(acc_val)\n",
    "                        \n",
    "            mean_loss = sum(prev_loss)/len(prev_loss)\n",
    "            mean_acc = sum(prev_acc)/len(prev_acc)\n",
    "\n",
    "            if epoch % display_steps == 0:\n",
    "                print(f\"{epoch}\\t\\t{round(loss_train, 3)}\\t\\t{round(loss_val, 3)}\\t\\t{acc_train}%\\t\\t{acc_val}%\")\n",
    "                #print(f\"LOSS: {epoch} - train: {loss_train}; val: {loss_val}; mean: {mean_loss}\")\n",
    "                #print(f\"ACC: {epoch} - train: {acc_train}; val: {acc_val}, mean: {mean_acc}\")\n",
    "            \n",
    "            if abs(mean_loss - loss_val) < self.epsilon:\n",
    "            #if abs(mean_acc - loss_val) < self.epsilon:\n",
    "                print(f\"Stopping early at epoch {epoch}\")\n",
    "                break\n",
    "            prev_loss.append(float(loss_val))\n",
    "            prev_acc.append(float(acc_val))\n",
    "            if len(prev_loss) > 10:\n",
    "                prev_loss.popleft()\n",
    "            if len(prev_acc) > 10:\n",
    "                prev_acc.popleft()\n",
    "\n",
    "            self.Lambda *= self.decay\n",
    "                \n",
    "    # Model Evaluation\n",
    "    def indicator(self, pred):\n",
    "        \"\"\"Returns label 1 if p(y == 1) > .5, 0 if p(y == 1) < .5, and breaks ties randomly\"\"\"\n",
    "        if pred > .5:\n",
    "            return 1\n",
    "        elif pred < .5:\n",
    "            return 0\n",
    "        return np.random.choice([0, 1])\n",
    "    \n",
    "    def get_pred_labels(self, preds):\n",
    "        \"\"\"Converts prediction probabilities into labels\"\"\"\n",
    "        for i in range(len(preds)):\n",
    "            preds[i] = self.indicator(preds[i])\n",
    "            \n",
    "        return preds\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"Compute the accuracy of the models predictions for test and training data\"\"\"\n",
    "        probs_train = self.predict(dataset=0)\n",
    "        acc_train, correct_train, total_train = self.accuracy(self.y_train, probs_train)\n",
    "        print(f\"TRAINING ACCURACY: {acc_train}%, {correct_train}/{total_train}\")\n",
    "        \n",
    "        probs_test = self.predict(dataset=1)\n",
    "        acc_test, correct_test, total_test = self.accuracy(self.y_test, probs_test)\n",
    "        print(f\"TESTING ACCURACY: {acc_test}%, {correct_test}/{total_test}\")\n",
    "\n",
    "        plot_data(f\"Loss In Relation to Epochs ({self.n} train samples)\", \"Epochs\", \"Loss\", [(self.train_losses, \"Train\"), (self.val_losses, \"Validation\")])\n",
    "        plot_data(f\"Accuracy In Relation to Epochs ({self.n} train samples)\", \"Epochs\", \"Accuracy\", [(self.train_accuracies, \"Train\"), (self.val_accuracies, \"Validation\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad5d7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and Preprocess Data\n",
    "data = DataSet()\n",
    "img_gen = ImageGenerator(5000, dataset = data, seed = 718067190)\n",
    "image_data, label_data = preprocess_data_plus(data.image_data, data.labels, \"convolution\")\n",
    "image_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa529d9d-8e5b-444e-8bf9-6c55c636ce20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set hyperparameters and train model\n",
    "lr = .01\n",
    "epsilon = .0002\n",
    "Lambda, decay = .01, .6\n",
    "EPOCH_LIM = 500\n",
    "regularization = (2, Lambda, decay)\n",
    "ttv_split = train_test_validation_split(image_data, label_data) # train, test, and validation\n",
    "\n",
    "logistic = LogisticRegression(*ttv_split, lr, epsilon, regularization)\n",
    "sgd = logistic.train_stochastic(EPOCH_LIM)\n",
    "predictions = logistic.get_pred_labels(logistic.predict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b8747b-9e69-4e69-b4cb-acf95a1b8858",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic.test()\n",
    "print(f\"SUM OF WEIGHTS: {sum(abs(logistic.weights))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87483dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in logistic.weights:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412a5457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression as SKLearnLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "diagrams = logistic.X_train\n",
    "labels = logistic.y_train\n",
    "\n",
    "diagrams2 = logistic.X_test\n",
    "labels2 = logistic.y_test\n",
    "\n",
    "print(diagrams.shape, labels.shape)\n",
    "print(diagrams2.shape, labels2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d4309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SKLearnLR(max_iter=500)\n",
    "model.fit(diagrams, labels)\n",
    "\n",
    "predictions = model.predict(diagrams2)\n",
    "\n",
    "accuracy = accuracy_score(labels2, predictions)\n",
    "print(\"Test accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fdb7e1-83e8-447e-8c7b-12f2736242b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Stashed Code\n",
    "Code that does not currently have use in the notebook, but could eventually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f24ee1-b564-4d42-9e6b-36955f424857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stashed Code\n",
    "# To reduce clutter in LogisticRegression class, I'm putting functions we currently have no need for here\n",
    "\n",
    "if False: # so this never gets executed\n",
    "    def preprocess_data(image_data, label_data):\n",
    "        \"\"\"Preprocess and encode image and label data for model training\"\"\"\n",
    "        image_data = one_hot_encode(image_data)\n",
    "        label_data = np.array(label_data)\n",
    "        flattened_data = image_data.reshape(image_data.shape[0], -1)\n",
    "        flattened_data = np.c_[np.ones(len(flattened_data)), flattened_data] \n",
    "        return flattened_data, label_data\n",
    "    \n",
    "    \n",
    "        \n",
    "    # Gradient Descent\n",
    "    def gd(self):\n",
    "        \"\"\"Run Gradient Descent to find `parameters` to minimize loss\"\"\"\n",
    "        # Shuffle data before each epoch\n",
    "        # random.shuffle(self.examples)\n",
    "        # for i in range(len(self.examples)):\n",
    "        #errors = self.loss(self.labels, self.predict())\n",
    "        residuals = self.predict() - self.y_train\n",
    "        gradient = np.dot(self.X_train.T, residuals)\n",
    "        self.weights -= self.lr * gradient\n",
    "    \n",
    "    \n",
    "    def train_deterministic(self, epochs):\n",
    "        \"\"\"Run GD until # of epochs is exceeded OR convergence\"\"\"\n",
    "        prev = float('inf')\n",
    "        for epoch in range(epochs):\n",
    "            self.gd()\n",
    "            train_loss = self.loss(self.y_train, self.predict())\n",
    "            if epoch % 5 == 0:\n",
    "                print(f\"{epoch} - Loss: {train_loss}\")\n",
    "                \n",
    "            if prev - train_loss < self.epsilon:\n",
    "                print(f\"Stopping early at epoch {epoch} - Loss: {train_loss}\")\n",
    "                break\n",
    "            prev = train_loss\n",
    "            \n",
    "        print(f\"{epoch} - Loss: {train_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
