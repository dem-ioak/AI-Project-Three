{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff11b85a-e34d-47e2-a032-04b20ff0b11e",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "All modules and packages required for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19fa14f-8d79-4759-8ec9-1f04bee37702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from enum import Enum\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from util.ImageGeneration import *\n",
    "from util.helper_functions import *\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aee4db",
   "metadata": {},
   "source": [
    "## Task 1 \n",
    "\n",
    "Build and train a model on the data set to take as input a wiring diagram and give as output whether or not it is dangerous.\n",
    "\n",
    "- How are you defining your input space?\n",
    "  - 20x20 image -> 400-dimension vector (one for each pixel)\n",
    "    - For each pixel, use one-hot encoding for each possible color (= 1600 features)\n",
    "  - Convolution layer converting image to 18x18 grid\n",
    "    - Use mean pooling on convolutional grid to get 9x9 grid \n",
    "      - For each cell, four possible colors (9x9x4 = 324 features)\n",
    "  - W_0 bias term (1 feature)\n",
    "  - 1600 + 324 + 1 = 1925 features\n",
    "- How are you defining your output space?\n",
    "  - Take sigmoid of the predictions (dot product of weights and data) to get a probability from 0 - 1.\n",
    "  - Use 0.5 as classification threshold (as half will be safe with the other half dangeous) to get a binary classification.\n",
    "  - 0 = Dangerous; 1 = Safe\n",
    "- What model space are you considering, and what parameters does it have? Be sure to specify any design choices you make here.\n",
    "  - Use Logistic Regression. \n",
    "  - Parameters: 1 Bias Term + 1600 raw features (cell and color) + 324 convolutional features\n",
    "    - We use convolutional features as it allows the model to better understand what is going on around a specific pixel. \n",
    "    - We used a cross shaped kernel to emphasize instances where a wire may intersect another wire, which would help in determining the situations where a diagram is dangerous\n",
    "      - The kernal was 3x3 as the model only needs to see its direct neighbors\n",
    "    - We then condense the data using mean pooling to understand how many colors may be in the surrounding area (better understand intersections)\n",
    "- How are you measuring the loss or error of a given model?\n",
    "  - Binary Cross Entropy/Log Loss\n",
    "- What training algorithm are you using to find the best model you can? Include any necessary math to specify your algorithm.\n",
    "  - Stochastic Gradient Descent. \n",
    "    - Take the derivative of the loss with respect to the weights for one specific datapoint.\n",
    "- How are you preventing overfitting?\n",
    "  - L2 Regularization to allow the gradient to explore different possible minima in the hopes to find the global minimum\n",
    "    - The penalty also has a decay so that as the model goes through more epochs, it stabilizes onto a specific minimum.\n",
    "  - Epsilon hyperparameter which measures how the losses are in respect to the number of epochs\n",
    "    - If the difference between the average of the last n losses (where n = 5) and the current loss was less than epsilon, we ended there.\n",
    "  - Train/Test/Validation split of 80/10/10 to prevent model overfitting to input data\n",
    "    - This ensures that the model is not only learning the data, but also generalizes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0955d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression - Task 1\n",
    "class LogisticRegression:\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, X_val, y_val, lr, epsilon, regularization, seed):\n",
    "        random.seed(seed)\n",
    "        \n",
    "        self.n = len(X_train)                                           # of training examples\n",
    "        self.d = len(X_train[0])                                        # of features\n",
    "        self.X_train = X_train #np.c_[np.ones(self.n), X_train]         # Training data\n",
    "        self.X_test = X_test                                            # Testing data\n",
    "        self.X_val = X_val                                              # Validation data\n",
    "        self.y_train = y_train                                          # Training classification Labels\n",
    "        self.y_test = y_test                                            # Testing classification Labels\n",
    "        self.y_val = y_val                                              # Validation classification Labels\n",
    "        self.weights = np.zeros(self.d)                                 # Current parameters / weights with d rows\n",
    "        self.lr = lr                                                    # Learning rate   \n",
    "        self.epsilon = epsilon                                          # Early stopping difference\n",
    "        self.regularization, self.Lambda, self.decay = regularization   # Type of regularization, penalty, and decay of the penalty\n",
    "\n",
    "    # Helper methods \n",
    "    # dataset = 0 - train; 1 - val; 2 - test\n",
    "    def dataset_picker(self, dataset = 0):\n",
    "        if dataset == 0:\n",
    "            return self.X_train, self.y_train\n",
    "        elif dataset == 1:\n",
    "            return self.X_test, self.y_test\n",
    "        else:\n",
    "            return self.X_val, self.y_val\n",
    "\n",
    "    # Helper methods \n",
    "    def predict(self, inds=None, dataset = 0):\n",
    "        \"\"\"Compute h_w(x_i) for the provided weight values\"\"\"\n",
    "        X, y = self.dataset_picker(dataset)\n",
    "        if inds is None:\n",
    "            inds = np.arange(len(X))\n",
    "        \n",
    "        dot_product = np.dot(X[inds], self.weights)\n",
    "        return sigmoid(dot_product)\n",
    "\n",
    "    def loss(self, y, p):\n",
    "        \"\"\"Compute the current value of average loss based on predictions\"\"\"\n",
    "        buffer = 1e-15\n",
    "        loss = np.mean(-y * np.log(p + buffer) - (1 - y) * np.log(1 - p + buffer))\n",
    "        if self.regularization == 2:\n",
    "            loss += np.sum(self.Lambda * np.square(self.weights))\n",
    "        return loss\n",
    "    \n",
    "    def accuracy(self, gold_labels, preds):\n",
    "        pred_labels = self.get_pred_labels(preds)\n",
    "        correct = [1 if pred == gold else 0 for pred, gold in zip(pred_labels, gold_labels)]\n",
    "        count, total = sum(correct), len(correct)\n",
    "        acc = round(count/total*100, 2)\n",
    "        \n",
    "        return acc, count, total\n",
    "    \n",
    "    def predict_loss_acc(self, inds=None, dataset=0):\n",
    "        X, y = self.dataset_picker(dataset)\n",
    "        preds = self.predict(inds, dataset)\n",
    "\n",
    "        loss = self.loss(y, preds)\n",
    "        acc, correct, total = self.accuracy(y, preds)\n",
    "        \n",
    "        return loss, acc\n",
    "            \n",
    "    def sgd(self):\n",
    "        \"\"\"Run a single iteration of SGD\"\"\"\n",
    "        # Shuffle data before each epoch\n",
    "        indices_array = np.arange(len(self.X_train))\n",
    "        random.shuffle(indices_array)\n",
    "        \n",
    "        for ind in indices_array:\n",
    "            residual = self.predict(ind) - self.y_train[ind]\n",
    "            gradient = residual * self.X_train[ind]\n",
    "            if self.regularization == 2:\n",
    "                gradient += 2 * self.Lambda * self.weights\n",
    "            self.weights -= self.lr * gradient\n",
    "\n",
    "    # Stochastic Gradient Descent\n",
    "    def train_stochastic(self, epochs, display_steps = 1):\n",
    "        \"\"\"Run SGD until # of epochs is exceeded OR convergence\"\"\"\n",
    "        prev_loss = deque([float('inf')])\n",
    "        prev_acc = deque([float('inf')])\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "        print(\"Epoch\\t\\tTrainLoss\\tValLoss\\t\\tTrainAcc\\tValAcc\")  \n",
    "        for epoch in range(epochs):\n",
    "            self.sgd()\n",
    "\n",
    "            loss_train, acc_train = self.predict_loss_acc(dataset=0)\n",
    "            loss_val, acc_val = self.predict_loss_acc(dataset=2)\n",
    "            \n",
    "            self.train_losses.append(loss_train)\n",
    "            self.val_losses.append(loss_val)\n",
    "            self.train_accuracies.append(acc_train)\n",
    "            self.val_accuracies.append(acc_val)\n",
    "                        \n",
    "            mean_loss = sum(prev_loss)/len(prev_loss)\n",
    "            mean_acc = sum(prev_acc)/len(prev_acc)\n",
    "\n",
    "            if epoch % display_steps == 0:\n",
    "                print(f\"{epoch}\\t\\t{round(loss_train, 3)}\\t\\t{round(loss_val, 3)}\\t\\t{acc_train}%\\t\\t{acc_val}%\")\n",
    "                #print(f\"LOSS: {epoch} - train: {loss_train}; val: {loss_val}; mean: {mean_loss}\")\n",
    "                #print(f\"ACC: {epoch} - train: {acc_train}; val: {acc_val}, mean: {mean_acc}\")\n",
    "            \n",
    "            if abs(mean_loss - loss_val) < self.epsilon:\n",
    "            #if abs(mean_acc - loss_val) < self.epsilon:\n",
    "                print(f\"Stopping early at epoch {epoch}\")\n",
    "                break\n",
    "            prev_loss.append(float(loss_val))\n",
    "            prev_acc.append(float(acc_val))\n",
    "            if len(prev_loss) > 10:\n",
    "                prev_loss.popleft()\n",
    "            if len(prev_acc) > 10:\n",
    "                prev_acc.popleft()\n",
    "\n",
    "            self.Lambda *= self.decay\n",
    "                \n",
    "    # Model Evaluation\n",
    "    def indicator(self, pred):\n",
    "        \"\"\"Returns label 1 if p(y == 1) > .5, 0 if p(y == 1) < .5, and breaks ties randomly\"\"\"\n",
    "        if pred > .5:\n",
    "            return 1\n",
    "        elif pred < .5:\n",
    "            return 0\n",
    "        return np.random.choice([0, 1])\n",
    "    \n",
    "    def get_pred_labels(self, preds):\n",
    "        \"\"\"Converts prediction probabilities into labels\"\"\"\n",
    "        for i in range(len(preds)):\n",
    "            preds[i] = self.indicator(preds[i])\n",
    "            \n",
    "        return preds\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"Compute the accuracy of the models predictions for test and training data\"\"\"\n",
    "        probs_train = self.predict(dataset=0)\n",
    "        acc_train, correct_train, total_train = self.accuracy(self.y_train, probs_train)\n",
    "        print(f\"TRAINING ACCURACY: {acc_train}%, {correct_train}/{total_train}\")\n",
    "        \n",
    "        probs_test = self.predict(dataset=1)\n",
    "        acc_test, correct_test, total_test = self.accuracy(self.y_test, probs_test)\n",
    "        print(f\"TESTING ACCURACY: {acc_test}%, {correct_test}/{total_test}\")\n",
    "\n",
    "        plot_data(f\"Loss In Relation to Epochs ({self.n} train samples)\", \"Epochs\", \"Loss\", [(self.train_losses, \"Train\"), (self.val_losses, \"Validation\")])\n",
    "        plot_data(f\"Accuracy In Relation to Epochs ({self.n} train samples)\", \"Epochs\", \"Accuracy\", [(self.train_accuracies, \"Train\"), (self.val_accuracies, \"Validation\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad5d7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and Preprocess Data\n",
    "data = DataSet()\n",
    "SEED = 718067190\n",
    "img_gen = ImageGenerator(5000, dataset = data, seed = SEED)\n",
    "image_data, label_data, _ = preprocess_data(data, \"noraw\")\n",
    "image_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa529d9d-8e5b-444e-8bf9-6c55c636ce20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set hyperparameters and train model\n",
    "lr = .01\n",
    "epsilon = .00001\n",
    "Lambda, decay = .01, .6\n",
    "EPOCH_LIM = 500\n",
    "regularization = (2, Lambda, decay)\n",
    "ttv_split = train_test_validation_split(image_data, label_data) # train, test, and validation\n",
    "\n",
    "logistic = LogisticRegression(*ttv_split, lr, epsilon, regularization, seed = SEED)\n",
    "sgd = logistic.train_stochastic(EPOCH_LIM)\n",
    "predictions = logistic.get_pred_labels(logistic.predict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b8747b-9e69-4e69-b4cb-acf95a1b8858",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic.test()\n",
    "print(f\"SUM OF WEIGHTS: {sum(abs(logistic.weights))}\")\n",
    "print(len(logistic.weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb3e95b",
   "metadata": {},
   "source": [
    "### Using scikit-learn's Logistic Regression Model for performance comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412a5457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression as SKLearnLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "diagrams = logistic.X_train\n",
    "labels = logistic.y_train\n",
    "\n",
    "diagrams2 = logistic.X_test\n",
    "labels2 = logistic.y_test\n",
    "\n",
    "print(diagrams.shape, labels.shape)\n",
    "print(diagrams2.shape, labels2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d4309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SKLearnLR(max_iter=500)\n",
    "model.fit(diagrams, labels)\n",
    "\n",
    "predictions = model.predict(diagrams2)\n",
    "\n",
    "accuracy = accuracy_score(labels2, predictions)\n",
    "print(\"Test accuracy: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
