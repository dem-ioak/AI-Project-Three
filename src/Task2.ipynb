{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff11b85a-e34d-47e2-a032-04b20ff0b11e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Dependencies\n",
    "All modules and packages required for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19fa14f-8d79-4759-8ec9-1f04bee37702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from enum import Enum\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from util.ImageGeneration import *\n",
    "from util.helper_functions import *\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10323d2",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ec7d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax Regression - Task 2\n",
    "class SoftmaxRegression:\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, X_val, y_val, lr, epsilon, regularization, seed):\n",
    "        random.seed(seed)\n",
    "        \n",
    "        self.n = len(X_train)                                           # of training examples\n",
    "        self.d = len(X_train[0])                                        # of features\n",
    "        self.labels = 4                                                 # of classes (colors)\n",
    "        self.X_train = X_train #np.c_[np.ones(self.n), X_train]         # Training data\n",
    "        self.X_test = X_test                                            # Testing data\n",
    "        self.X_val = X_val                                              # Validation data\n",
    "        self.y_train = y_train                                          # Training classification Labels\n",
    "        self.y_test = y_test                                            # Testing classification Labels\n",
    "        self.y_val = y_val                                              # Validation classification Labels\n",
    "        self.weights = np.zeros((self.labels, self.d))                  # Current parameters / weights with d rows\n",
    "        self.lr = lr                                                    # Learning rate   \n",
    "        self.epsilon = epsilon                                          # Early stopping difference\n",
    "        self.regularization, self.Lambda, self.decay = regularization   # Type of regularization, penalty, and decay of the penalty\n",
    "\n",
    "    # Helper methods \n",
    "    # dataset = 0 - train; 1 - val; 2 - test\n",
    "    def dataset_picker(self, dataset = 0):\n",
    "        if dataset == 0:\n",
    "            return self.X_train, self.y_train\n",
    "        elif dataset == 1:\n",
    "            return self.X_test, self.y_test\n",
    "        else:\n",
    "            return self.X_val, self.y_val\n",
    "\n",
    "    # Helper methods \n",
    "    def predict(self, inds=None, dataset = 0):\n",
    "        \"\"\"Compute h_w(x_i) for the provided weight values\"\"\"\n",
    "        X, y = self.dataset_picker(dataset)\n",
    "        if inds is None:\n",
    "            inds = np.arange(len(X))\n",
    "        \n",
    "        dot_product = np.dot(self.weights, X[inds].T)\n",
    "        return softmax(dot_product)\n",
    "\n",
    "    def loss(self, Y, P):\n",
    "        \"\"\"Compute the current value of average loss based on predictions\"\"\"\n",
    "        buffer = 1e-15\n",
    "        #print(Y.shape, P.shape)\n",
    "        loss = np.mean(-Y.T * np.log(P + buffer))\n",
    "        if self.regularization == 2:\n",
    "            loss += np.sum(self.Lambda * np.square(self.weights))\n",
    "        return loss\n",
    "    \n",
    "    def accuracy(self, gold_labels, preds):\n",
    "        #print(preds.shape)\n",
    "        pred_labels = self.get_pred_labels(preds)\n",
    "        correct = [1 if np.array_equal(pred, gold) else 0 for pred, gold in zip(pred_labels.T, gold_labels)]\n",
    "        count, total = sum(correct), len(correct)\n",
    "        acc = round(count/total*100, 2)\n",
    "        \n",
    "        return acc, count, total\n",
    "    \n",
    "    def predict_loss_acc(self, inds=None, dataset=0):\n",
    "        X, y = self.dataset_picker(dataset)\n",
    "        preds = self.predict(inds, dataset)\n",
    "\n",
    "        loss = self.loss(y, preds)\n",
    "        acc, correct, total = self.accuracy(y, preds)\n",
    "        \n",
    "        return loss, acc\n",
    "    \n",
    "    def sgd(self):\n",
    "        \"\"\"Run a single epoch of SGD\"\"\"\n",
    "        # Shuffle data before each epoch\n",
    "        indices_array = np.arange(len(self.X_train))\n",
    "        random.shuffle(indices_array)\n",
    "        \n",
    "        for ind in indices_array:\n",
    "            residual = self.predict(ind) - self.y_train[ind]\n",
    "            residual = residual[:, np.newaxis]\n",
    "            x_col = self.X_train[ind][:, np.newaxis].T\n",
    "            gradient = np.dot(residual, x_col)\n",
    "            \n",
    "            if self.regularization == 2:\n",
    "                gradient += 2 * self.Lambda * self.weights\n",
    "            self.weights -= self.lr * gradient\n",
    "\n",
    "    # Stochastic Gradient Descent\n",
    "    def train(self, epochs, display_steps = 1, stochastic=True):\n",
    "        \"\"\"Run SGD until # of epochs is exceeded OR convergence\"\"\"\n",
    "        prev_loss = deque([float('inf')])\n",
    "        prev_acc = deque([float('inf')])\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "        print(\"Epoch\\t\\tTrainLoss\\tValLoss\\t\\tTrainAcc\\tValAcc\")  \n",
    "        for epoch in range(epochs):\n",
    "            if stochastic:\n",
    "                self.sgd()\n",
    "            else: self.gd()\n",
    "\n",
    "            loss_train, acc_train = self.predict_loss_acc(dataset=0)\n",
    "            loss_val, acc_val = self.predict_loss_acc(dataset=2)\n",
    "            \n",
    "            self.train_losses.append(loss_train)\n",
    "            self.val_losses.append(loss_val)\n",
    "            self.train_accuracies.append(acc_train)\n",
    "            self.val_accuracies.append(acc_val)\n",
    "                        \n",
    "            mean_loss = sum(prev_loss)/len(prev_loss)\n",
    "            mean_acc = sum(prev_acc)/len(prev_acc)\n",
    "\n",
    "            if epoch % display_steps == 0:\n",
    "                print(f\"{epoch}\\t\\t{round(loss_train, 3)}\\t\\t{round(loss_val, 3)}\\t\\t{acc_train}%\\t\\t{acc_val}%\")\n",
    "                #print(f\"LOSS: {epoch} - train: {loss_train}; val: {loss_val}; mean: {mean_loss}\")\n",
    "                #print(f\"ACC: {epoch} - train: {acc_train}; val: {acc_val}, mean: {mean_acc}\")\n",
    "            \n",
    "            if abs(mean_loss - loss_val) < self.epsilon:\n",
    "            #if abs(mean_acc - loss_val) < self.epsilon:\n",
    "                print(f\"Stopping early at epoch {epoch}\")\n",
    "                break\n",
    "            prev_loss.append(float(loss_val))\n",
    "            prev_acc.append(float(acc_val))\n",
    "            if len(prev_loss) > 10:\n",
    "                prev_loss.popleft()\n",
    "            if len(prev_acc) > 10:\n",
    "                prev_acc.popleft()\n",
    "\n",
    "            self.Lambda *= self.decay\n",
    "                \n",
    "    # Model Evaluation\n",
    "    def indicator(self, pred):\n",
    "        \"\"\"Returns label 1 if p(y == 1) > .5, 0 if p(y == 1) < .5, and breaks ties randomly\"\"\"\n",
    "        max_ind = np.argmax(pred)\n",
    "        one_hot = np.zeros_like(pred)\n",
    "        one_hot[max_ind] = 1\n",
    "        return one_hot\n",
    "    \n",
    "    def get_pred_labels(self, preds):\n",
    "        \"\"\"Converts prediction probabilities into labels\"\"\"\n",
    "        for i in range(preds.shape[1]):\n",
    "            #print(preds[:,i].shape, preds[:,i])\n",
    "            preds[:,i] = self.indicator(preds[:,i])\n",
    "            \n",
    "        return preds\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"Compute the accuracy of the models predictions for test and training data\"\"\"\n",
    "        probs_train = self.predict(dataset=0)\n",
    "        acc_train, correct_train, total_train = self.accuracy(self.y_train, probs_train)\n",
    "        print(f\"TRAINING ACCURACY: {acc_train}%, {correct_train}/{total_train}\")\n",
    "        \n",
    "        probs_test = self.predict(dataset=1)\n",
    "        acc_test, correct_test, total_test = self.accuracy(self.y_test, probs_test)\n",
    "        print(f\"TESTING ACCURACY: {acc_test}%, {correct_test}/{total_test}\")\n",
    "\n",
    "        plot_data(f\"Loss In Relation to Epochs ({self.n} train samples)\", \"Epochs\", \"Loss\", [(self.train_losses, \"Train\"), (self.val_losses, \"Validation\")])\n",
    "        plot_data(f\"Accuracy In Relation to Epochs ({self.n} train samples)\", \"Epochs\", \"Accuracy\", [(self.train_accuracies, \"Train\"), (self.val_accuracies, \"Validation\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4594b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and Preprocess Data\n",
    "data = DataSet()\n",
    "SEED = 718067190\n",
    "img_gen = ImageGenerator(5000, dataset = data, seed = SEED, task = 2)\n",
    "image_data, _, third_wires = preprocess_data(data, \"both\")\n",
    "image_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0257c225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters and train Model\n",
    "lr = .01\n",
    "epsilon = .0001\n",
    "Lambda, decay = .01, .6\n",
    "EPOCH_LIM = 500\n",
    "regularization = (2, Lambda, decay)\n",
    "ttv_split = train_test_validation_split(image_data, third_wires) # train, test, and validation\n",
    "\n",
    "softm = SoftmaxRegression(*ttv_split, lr, epsilon, regularization, seed = SEED)\n",
    "sgd = softm.train(EPOCH_LIM)\n",
    "predictions = softm.get_pred_labels(softm.predict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2d9173",
   "metadata": {},
   "outputs": [],
   "source": [
    "softm.test()\n",
    "print(f\"SUM OF WEIGHTS: {np.sum(abs(softm.weights))}\")\n",
    "print(softm.weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3538e294",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = softm.predict()\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6ce461",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
